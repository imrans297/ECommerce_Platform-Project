alerts:
  - name: "High Error Rate"
    type: "metric alert"
    query: "avg(last_5m):sum:trace.http.request.errors{env:production} by {service}.as_rate() > 0.05"
    message: |
      Service {{service.name}} has high error rate: {{value}}%
      @slack-alerts @pagerduty-service
    tags:
      - "team:platform"
      - "severity:high"
    
  - name: "High Response Time"
    type: "metric alert"
    query: "avg(last_5m):p95:trace.http.request.duration{env:production} by {service} > 2"
    message: |
      Service {{service.name}} response time is high: {{value}}s
      @slack-alerts
    tags:
      - "team:platform"
      - "severity:medium"
      
  - name: "Service Down"
    type: "service check"
    query: '"http_check".over("service").by("service").last(2).count_by_status()'
    message: |
      Service {{service.name}} is down!
      @slack-alerts @pagerduty-service
    tags:
      - "team:platform"
      - "severity:critical"
      
  - name: "High CPU Usage"
    type: "metric alert"
    query: "avg(last_10m):avg:kubernetes.cpu.usage.total{cluster_name:ecommerce-platform} by {pod_name} > 80"
    message: |
      Pod {{pod_name.name}} CPU usage is high: {{value}}%
      @slack-alerts
    tags:
      - "team:platform"
      - "severity:medium"